[34m[1mwandb[0m: Detected [openai] in use.
[34m[1mwandb[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[34m[1mwandb[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/
Starting warmup run with 10 episodes...
Traceback (most recent call last):
  File "/home/fred/code/memory/free_bao/src/free_bao/main.py", line 47, in <module>
    main()
    ~~~~^^
  File "/home/fred/code/memory/free_bao/src/free_bao/main.py", line 22, in main
    runner.run_benchmark(num_episodes=args.episodes, mode=args.benchmark_mode)
    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/fred/code/memory/free_bao/src/free_bao/simulation/benchmark.py", line 61, in run_benchmark
    result = app.invoke({"messages": current_messages, "task": task})
  File "/home/fred/code/memory/free_bao/.venv/lib/python3.13/site-packages/langgraph/pregel/main.py", line 3071, in invoke
    for chunk in self.stream(
                 ~~~~~~~~~~~^
        input,
        ^^^^^^
    ...<10 lines>...
        **kwargs,
        ^^^^^^^^^
    ):
    ^
  File "/home/fred/code/memory/free_bao/.venv/lib/python3.13/site-packages/langgraph/pregel/main.py", line 2646, in stream
    for _ in runner.tick(
             ~~~~~~~~~~~^
        [t for t in loop.tasks.values() if not t.writes],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<2 lines>...
        schedule_task=loop.accept_push,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ):
    ^
  File "/home/fred/code/memory/free_bao/.venv/lib/python3.13/site-packages/langgraph/pregel/_runner.py", line 167, in tick
    run_with_retry(
    ~~~~~~~~~~~~~~^
        t,
        ^^
    ...<10 lines>...
        },
        ^^
    )
    ^
  File "/home/fred/code/memory/free_bao/.venv/lib/python3.13/site-packages/langgraph/pregel/_retry.py", line 42, in run_with_retry
    return task.proc.invoke(task.input, config)
           ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^
  File "/home/fred/code/memory/free_bao/.venv/lib/python3.13/site-packages/langgraph/_internal/_runnable.py", line 656, in invoke
    input = context.run(step.invoke, input, config, **kwargs)
  File "/home/fred/code/memory/free_bao/.venv/lib/python3.13/site-packages/langgraph/_internal/_runnable.py", line 400, in invoke
    ret = self.func(*args, **kwargs)
  File "/home/fred/code/memory/free_bao/src/free_bao/agent/react_agent.py", line 64, in reason
    response = self.llm.invoke(messages)
  File "/home/fred/code/memory/free_bao/.venv/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 5695, in invoke
    return self.bound.invoke(
           ~~~~~~~~~~~~~~~~~^
        input,
        ^^^^^^
        self._merge_configs(config),
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        **{**self.kwargs, **kwargs},
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/fred/code/memory/free_bao/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py", line 402, in invoke
    self.generate_prompt(
    ~~~~~~~~~~~~~~~~~~~~^
        [self._convert_input(input)],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<6 lines>...
        **kwargs,
        ^^^^^^^^^
    ).generations[0][0],
    ^
  File "/home/fred/code/memory/free_bao/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py", line 1121, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/fred/code/memory/free_bao/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py", line 931, in generate
    self._generate_with_cache(
    ~~~~~~~~~~~~~~~~~~~~~~~~~^
        m,
        ^^
    ...<2 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "/home/fred/code/memory/free_bao/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py", line 1233, in _generate_with_cache
    result = self._generate(
        messages, stop=stop, run_manager=run_manager, **kwargs
    )
  File "/home/fred/code/memory/free_bao/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py", line 1473, in _generate
    _handle_openai_api_error(e)
    ~~~~~~~~~~~~~~~~~~~~~~~~^^^
  File "/home/fred/code/memory/free_bao/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py", line 1468, in _generate
    raw_response = self.client.with_raw_response.create(**payload)
  File "/home/fred/code/memory/free_bao/.venv/lib/python3.13/site-packages/openai/_legacy_response.py", line 364, in wrapped
    return cast(LegacyAPIResponse[R], func(*args, **kwargs))
                                      ~~~~^^^^^^^^^^^^^^^^^
  File "/home/fred/code/memory/free_bao/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
  File "/home/fred/code/memory/free_bao/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1192, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<47 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/fred/code/memory/free_bao/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1297, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/fred/code/memory/free_bao/.venv/lib/python3.13/site-packages/openai/_base_client.py", line 1070, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}
During task with name 'reason' and id 'c605bd66-e27c-5377-36e1-1326b39ef115'
